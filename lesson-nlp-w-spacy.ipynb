{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "097ce07c-2890-4eae-b381-d0760ef68156",
   "metadata": {},
   "source": [
    "# NLP with SpaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee0c077-edec-45ea-ba8b-3ecb2a451fda",
   "metadata": {},
   "source": [
    "## Import SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "912e02a2-f79b-40b5-966e-0a82408457b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x200284c0b50>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c596f63-5bc3-44e2-aa3b-64c1a8112729",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nlp encapsulates the entire nlp pipeline\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2116ef29-f4fd-4698-859c-274168f3e6d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "While running in Central Park, \n",
      "I noticed a discarded McDonald's container,surounded by buzzing flies was annoying.\n"
     ]
    }
   ],
   "source": [
    "# sample text for doc demo\n",
    "# define text for demonstration\n",
    "sample_text = \"While running in Central Park, \\nI noticed a discarded McDonald's container,surounded by buzzing flies was annoying.\"\n",
    "print(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b9a8c7-382f-47ed-92ab-31d6faf2e6d0",
   "metadata": {},
   "source": [
    "### Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b462169a-ec96-4183-bea2-2e02e51b7b6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a doc with the nlp pipeline\n",
    "doc = nlp(sample_text)\n",
    "type(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c49609c-81fd-48cb-93f2-98391721af42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "While running in Central Park, \n",
      "I noticed a discarded McDonald's container,surounded by buzzing flies was annoying.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "While running in Central Park, \n",
       "I noticed a discarded McDonald's container,surounded by buzzing flies was annoying."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# docs look like original text when displayed\n",
    "print(sample_text)\n",
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab979eeb-e8f3-4ecc-bcef-167b6f861eae",
   "metadata": {},
   "source": [
    "### Token objects within docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77893135-69b7-4b47-a0ff-1b3244697f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "While\n",
      "running\n",
      "in\n",
      "Central\n",
      "Park\n",
      ",\n",
      "\n",
      "\n",
      "I\n",
      "noticed\n",
      "a\n",
      "discarded\n",
      "McDonald\n",
      "'s\n",
      "container\n",
      ",\n",
      "surounded\n",
      "by\n",
      "buzzing\n",
      "flies\n",
      "was\n",
      "annoying\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c18416d-f68e-43a3-b19d-4b2ca8deb138",
   "metadata": {},
   "source": [
    "## Token Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbe930db-f281-4fa0-900c-598063cd12a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "running"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# slice token from doc\n",
    "token = doc[1]\n",
    "token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9967508c-5ff7-4fd4-90d3-8322d33d952a",
   "metadata": {},
   "source": [
    "### token.text: original form of the word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5bd54cc7-8156-4bea-8c2d-b83d1ee98316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running\n"
     ]
    }
   ],
   "source": [
    "print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b5fc42-f3c2-45a7-9c24-b859bcba2f9a",
   "metadata": {},
   "source": [
    "### token.lemma_: the base or root form of the word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be698b2d-000f-462b-8d6d-5b5f47b94517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n"
     ]
    }
   ],
   "source": [
    "print(token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ff6958-9563-4c97-bb90-563196b7761e",
   "metadata": {},
   "source": [
    "### token.pos_: The part-of-speech tag associated with the token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28aa672d-c092-4975-9803-4590d4c027e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VERB\n"
     ]
    }
   ],
   "source": [
    "print(token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0513eaec-fb9f-4f78-addf-868668ef1101",
   "metadata": {},
   "source": [
    "###  token.is_stop: Boolean flag to check if the token is a stop word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d3dcaf2-147d-4a1b-98c8-3d32b65b2890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101e1e75-1374-4268-9c3c-13a046346b24",
   "metadata": {},
   "source": [
    "### token.is_punct: Boolean flag to check if the token is punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "702070dc-44f2-4f61-a67a-56eefce42c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(token.is_punct)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671435a5-b4d3-44b6-8ce8-49e608a88a5b",
   "metadata": {},
   "source": [
    "### loop through each token, create dict for each token, convert to df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7915673-9d21-471a-a606-e68f55dfa20b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>.text</th>\n",
       "      <th>.lemma_</th>\n",
       "      <th>.pos_</th>\n",
       "      <th>.is_stop</th>\n",
       "      <th>.is_punct</th>\n",
       "      <th>.is_space</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>While</td>\n",
       "      <td>while</td>\n",
       "      <td>SCONJ</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>running</td>\n",
       "      <td>run</td>\n",
       "      <td>VERB</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>in</td>\n",
       "      <td>in</td>\n",
       "      <td>ADP</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Central</td>\n",
       "      <td>Central</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Park</td>\n",
       "      <td>Park</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\\n</td>\n",
       "      <td>\\n</td>\n",
       "      <td>SPACE</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I</td>\n",
       "      <td>I</td>\n",
       "      <td>PRON</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>noticed</td>\n",
       "      <td>notice</td>\n",
       "      <td>VERB</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>DET</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     .text  .lemma_  .pos_  .is_stop  .is_punct  .is_space\n",
       "0    While    while  SCONJ      True      False      False\n",
       "1  running      run   VERB     False      False      False\n",
       "2       in       in    ADP      True      False      False\n",
       "3  Central  Central  PROPN     False      False      False\n",
       "4     Park     Park  PROPN     False      False      False\n",
       "5        ,        ,  PUNCT     False       True      False\n",
       "6       \\n       \\n  SPACE     False      False       True\n",
       "7        I        I   PRON      True      False      False\n",
       "8  noticed   notice   VERB     False      False      False\n",
       "9        a        a    DET      True      False      False"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Create dictionary for desired attributes for each token\n",
    "token_data = []\n",
    "for token in doc:\n",
    "    token_dict = {\n",
    "        \".text\": token.text,\n",
    "        \".lemma_\": token.lemma_,\n",
    "        \".pos_\": token.pos_,\n",
    "        \".is_stop\": token.is_stop,\n",
    "        \".is_punct\": token.is_punct,\n",
    "        \".is_space\": token.is_space\n",
    "    }\n",
    "    token_data.append(token_dict)\n",
    "# Save dictionary as a dataframe\n",
    "spacy_df = pd.DataFrame(token_data) \n",
    "spacy_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7aa4ec-746e-47db-9dd8-dcbc8e4ad60c",
   "metadata": {},
   "source": [
    "## Preprocessing with SpaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc00111-41ab-42c8-8632-96ef8fefaabf",
   "metadata": {},
   "source": [
    "### Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e6024ba-d3ae-45e6-962d-f598be9abdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty list to collect tokens after cleaning\n",
    "cleaned_tokens = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4d6b02d7-8aa5-4e84-aa1b-b20e018b1d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['running', 'central', 'park', ',', '\\n', 'noticed', 'discarded', 'mcdonald', 'container', ',', 'surounded', 'buzzing', 'flies', 'annoying', '.']\n"
     ]
    }
   ],
   "source": [
    "# loop to remove stop words\n",
    "for token in doc:\n",
    "    if token.is_stop == True:\n",
    "        continue\n",
    "    else:\n",
    "        cleaned_tokens.append(token.text.lower())\n",
    "print(cleaned_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7e8f86-260a-47ff-ace1-6d2ea1067ee7",
   "metadata": {},
   "source": [
    "### Remove punctuation and whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e2901779-32d3-4eb0-a9f9-ad27bf61231a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['running', 'central', 'park', 'noticed', 'discarded', 'mcdonald', 'container', 'surounded', 'buzzing', 'flies', 'annoying']\n"
     ]
    }
   ],
   "source": [
    "## Adding onto our preprocessing for loop\n",
    "# For loop to remove stopwords & punctuation\n",
    "cleaned_tokens = []\n",
    "# For each token \n",
    "for token in doc:\n",
    "    \n",
    "    # If the token is a stopword,\n",
    "    if token.is_stop == True:\n",
    "        # skip it and move onto next token\n",
    "        continue \n",
    "    ##NEW: \n",
    "    # if the token is punctuation,\n",
    "    if token.is_punct == True:\n",
    "        # skip it and move onto next token\n",
    "        continue\n",
    "    # if the token is a whitespace  (spaces, new lines, etc)\n",
    "    if token.is_space == True:\n",
    "        # skip it and move onto next token\n",
    "        continue\n",
    "    \n",
    "    # Otherwise,\n",
    "    else: \n",
    "        # keep the tokens'.text for the final list of tokens\n",
    "        cleaned_tokens.append(token.text.lower())\n",
    "        \n",
    "print(cleaned_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b01de6-8224-4eb8-a9b4-0601d6029677",
   "metadata": {},
   "source": [
    "### obtain lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5499b71f-e6f6-4552-b3f5-91daef87bbf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run', 'central', 'park', 'notice', 'discard', 'mcdonald', 'container', 'surounde', 'buzz', 'fly', 'annoying']\n"
     ]
    }
   ],
   "source": [
    "## Adding onto our preprocessing for loop\n",
    "# For loop to remove stopwords & punctuation\n",
    "cleaned_lemmas = []\n",
    "# For each token \n",
    "for token in doc:\n",
    "    \n",
    "    # If the token is a stopword,\n",
    "    if token.is_stop == True:\n",
    "        # skip it and move onto next token\n",
    "        continue \n",
    "    \n",
    "    # if the token is punctuation,\n",
    "    if token.is_punct == True:\n",
    "        # skip it and move onto next token\n",
    "        continue\n",
    "    # if the token is a whitespace  (spaces, new lines, etc)\n",
    "    if token.is_space == True:\n",
    "        # skip it and move onto next token\n",
    "        continue\n",
    "    \n",
    "    # Otherwise,\n",
    "    else: \n",
    "        # # keep the tokens'.text for the final list of tokens\n",
    "        # cleaned_tokens.append(token.text.lower())\n",
    "        # keep the tokens's .lemma_ for the final list of tokens\n",
    "        cleaned_lemmas.append(token.lemma_.lower())\n",
    "        \n",
    "print(cleaned_lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbc9a68-a43b-44cf-9b09-823ba689dc8f",
   "metadata": {},
   "source": [
    "### compare cleaned tokens vs. cleaned lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "87df449b-4d1a-4ae8-97a3-123c6cc7b54b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized words:\n",
      " ['running', 'central', 'park', 'noticed', 'discarded', 'mcdonald', 'container', 'surounded', 'buzzing', 'flies', 'annoying'] \n",
      "\n",
      "Lemmatized words:\n",
      " ['run', 'central', 'park', 'notice', 'discard', 'mcdonald', 'container', 'surounde', 'buzz', 'fly', 'annoying']\n"
     ]
    }
   ],
   "source": [
    "# Compare text and lemmas\n",
    "print(\"Tokenized words:\\n\", cleaned_tokens,\"\\n\")\n",
    "print(\"Lemmatized words:\\n\", cleaned_lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797a2f2c-89ae-467b-a13e-dcf771764de4",
   "metadata": {},
   "source": [
    "## Define function for preprocessing with SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "00500a0d-ff1a-40ed-8cd2-d01c06f3a3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function\n",
    "\n",
    "def preprocess_doc(doc, remove_stopwords=True, remove_punct=True, use_lemmas=False):\n",
    "    \"\"\"Temporary Fucntion - for Education Purposes (we will make something better below)\n",
    "    \"\"\"\n",
    "    tokens = [ ]\n",
    "    for token in doc:\n",
    "        # Check if should remove stopwords and if token is stopword\n",
    "        if (remove_stopwords == True) and (token.is_stop == True):\n",
    "            # Continue the loop with the next token\n",
    "            continue\n",
    "    \n",
    "        # Check if should remove stopwords and if token is stopword\n",
    "        if (remove_punct == True) and (token.is_punct == True):\n",
    "            continue\n",
    "    \n",
    "        # Check if should remove stopwords and if token is stopword\n",
    "        if (remove_punct == True) and (token.is_space == True):\n",
    "            continue\n",
    "    \n",
    "        ## Determine final form of output list of tokens/lemmas\n",
    "        if use_lemmas:\n",
    "            tokens.append(token.lemma_.lower())\n",
    "        else:\n",
    "            tokens.append(token.text.lower())\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b7c18518-8e10-46e1-ba8a-66eb6bf00ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['while', 'running', 'in', 'central', 'park', ',', '\\n', 'i', 'noticed', 'a', 'discarded', 'mcdonald', \"'s\", 'container', ',', 'surounded', 'by', 'buzzing', 'flies', 'was', 'annoying', '.']\n"
     ]
    }
   ],
   "source": [
    "# run function with false args\n",
    "\n",
    "# Convert the text to a doc.\n",
    "doc = nlp(sample_text)\n",
    "# Tokenizing, keeping stopwords and punctuatin\n",
    "dirty_tokens = preprocess_doc(doc, remove_stopwords=False,remove_punct=False)\n",
    "print(dirty_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bc782770-86ce-4d58-b9af-cc484b0c37d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['running', 'central', 'park', 'noticed', 'discarded', 'mcdonald', 'container', 'surounded', 'buzzing', 'flies', 'annoying']\n"
     ]
    }
   ],
   "source": [
    "# run function with true args\n",
    "# Tokenizing, removing stopwords and punctuation\n",
    "cleaned_tokens = preprocess_doc(doc, remove_stopwords=True,remove_punct=True)\n",
    "print(cleaned_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2551df-6075-4b78-bdad-10730727b3b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dojo-env)",
   "language": "python",
   "name": "dojo-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
